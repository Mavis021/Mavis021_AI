{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative Adversarial Networks (GAN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adversarial Learning\n",
    "\n",
    "* Adversarial machine learning is a technique used in machine learning (ML) to fool or misguide a model with malicious input.\n",
    "\n",
    "* It includes both the generation and detection of adversarial examples, which are inputs specially created to deceive classifiers.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generative Models\n",
    "\n",
    "* Models that can generate new data samples from the learned data distribution.\n",
    "\n",
    "* If we focus on images, then generative models learn the pixel intensity distribution. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [GAN](https://arxiv.org/abs/1406.2661)\n",
    "\n",
    "* GAN is the **GENERATIVE**, **ADVERSARIAL** **UNSUPERVISED** learning algorithm\n",
    "\n",
    "* GAN consists of two different AI models viz. Generator Network and Discriminator Network.\n",
    "\n",
    "* Generator and Discriminator Networks compete against each other hence following adversarial learning.\n",
    "\n",
    "* Working Mechanism Intuition:\n",
    "    * Generator generates new data samples. A well trained generator generates realistic data (images in our case).\n",
    "\n",
    "    * Discriminator is a classifier that classifes whether the input data(image) is a real one(i.e. images from training set) or a fake one (i.e. generated by generator).\n",
    "    \n",
    "    * Good discriminator can classify real and fake samples with good accuracy.\n",
    "\n",
    "    * Good generator can create realistic fake samples and can fool discriminator.\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GAN Architecture\n",
    "![gan-arch](./assets/gan-arch.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's Build GAN by Ourselves\n",
    "\n",
    "* Building GANs mean building two neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell-1\n",
    "# Import necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation\n",
    "* We will be using MNIST dataset which can be easily downloaded from torchvision-datasets.\n",
    "\n",
    "* We will be using only images from that dataset not labels (unsupervised learning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell-2\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualization of MNIST Dataset Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell-3\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5  # Denormalize the image\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)), cmap='gray')\n",
    "    plt.show()\n",
    "\n",
    "def show_grid(images):\n",
    "    img_grid = torchvision.utils.make_grid(images, nrow=8)\n",
    "    imshow(img_grid)\n",
    "\n",
    "# Get some random training images\n",
    "trainloader_for_vis = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "dataiter = iter(trainloader_for_vis)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# Display the images in a grid\n",
    "show_grid(images)\n",
    "\n",
    "print(f\"Shape of a batch of images: {images.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Discriminator Network\n",
    "\n",
    "* Discriminator is a classfier.\n",
    "\n",
    "* Input: Image\n",
    "\n",
    "* Ouput: Real or Fake\n",
    "\n",
    "* We will be building a simple discriminator using Mulit-layer Perceptron.\n",
    "\n",
    "* Do not forget to flatten the image since we will be using linear layers.\n",
    "\n",
    "* Keep the architecture as simple as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell-4\n",
    "# Your code here\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            ?\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = ?\n",
    "        x = ?\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Generator Network\n",
    "\n",
    "* We will be building simple generator network which is also a multi-layer perceptron network.\n",
    "\n",
    "* Input to the generator will be a vector which is known as latent vector or noise vector.\n",
    "\n",
    "* The dimension of latent vector is a hyperparameter. Let's use latent vector of dimension 100.\n",
    "\n",
    "* Latent vector is usually sampled from normal distribution.\n",
    "\n",
    "* Output of the generator will be a vector of dimension equal to training image dimension.\n",
    "\n",
    "* Do not forget to reshape the output vector to 2d-image shape same as training image dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell-5\n",
    "# Your code here\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim=100):\n",
    "        super(Generator, self).__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            ?\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = ?\n",
    "        x = x.reshape(x.size(0), 1, 28, 28)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective Function for GAN\n",
    "\n",
    "**Discriminator Objective Function**\n",
    "\n",
    "* Discriminator objective is to classify real image as 1 and fake image as 0.\n",
    "\n",
    "* Discriminator is trained with both real images and fake images.\n",
    "\n",
    "* Discriminator loss consists of two parts:\n",
    "\n",
    "    1. real_data_loss: The loss when the discriminator correctly identifies real data as real.\n",
    "\n",
    "    2. fake_data_loss: The loss when the discriminator correctly identifies fake data as fake.\n",
    "\n",
    "* Since discriminator formulates binary classification problem, both real_data_loss and fake_data_loss are BinaryCrossEntropy Loss.\n",
    "\n",
    "* Total discriminator loss = real_data_loss + fake_data_loss\n",
    "\n",
    "* Objective: **Minimize(real_data_loss + fake_data_loss)**\n",
    "\n",
    "\n",
    "**Generator Objective Function**\n",
    "* Generator objective is to generate images that looks realistic. In other words, generator objective is to fool discriminator. In other words, generator objective is to increase discriminator loss and decrease discriminator accuracy.\n",
    "\n",
    "* If the image generated by generator is classified as fake by discriminator, then generator_loss should increase. (In this scenario, generator fails to achieve its objective of fooling discriminator)\n",
    "\n",
    "* If the image generated by generator is classified as real by discriminator, then generator_loss should decrease. (In this scenario, generator succeed to achieve its objective of fooling discriminator)\n",
    "\n",
    "* We use BinaryCrossEntropy as loss for generator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training GAN\n",
    "\n",
    "* Training GAN involve training Discriminator and Generator alternatively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell-6\n",
    "# Using available hardware device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Creating the generator and discriminator\n",
    "latent_dim = 100\n",
    "generator = Generator(latent_dim).to(device)\n",
    "discriminator = Discriminator().to(device)\n",
    "\n",
    "# creating binary cross entropy loss instancea\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# creating optimizers for generator and discriminator\n",
    "optimizer_G = optim.Adam(generator.parameters(), lr=0.0002)\n",
    "optimizer_D = optim.Adam(discriminator.parameters(), lr=0.0002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell-7\n",
    "# Containers to store losses for visualization purpose\n",
    "num_epochs = 20\n",
    "discriminator_loss = []\n",
    "generator_loss = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps for Training Discriminator\n",
    "\n",
    "1. Infer discriminator with a mini-batch of real images from training set.\n",
    "2. Calculate loss for discriminator with real data.\n",
    "3. Generate latent vector from normal distribution for mini-batch.\n",
    "4. Generate fake image by feeding latent vector to generator.\n",
    "5. Infer discriminator with fake images.\n",
    "6. Calulate loss for discriminator with fake data.\n",
    "7. Compute total loss by adding both losses\n",
    "8. Compute the gradient of total discriminator loss and update the discriminator optimizer parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps for Training Generator\n",
    "1. Generate latent vector from normal distribution for mini-batch.\n",
    "2. Generate fake image by feeding latent vector to generator.\n",
    "3. Infer the discriminator with fake image.\n",
    "4. Calculate loss for generator.\n",
    "5. Compute the gradient of generator loss and update its optimizer parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell-8\n",
    "# Your code here\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    d_epoch_loss = 0\n",
    "    g_epoch_loss = 0\n",
    "    for i, (images, _) in enumerate(trainloader):\n",
    "        batch_size = images.size(0)\n",
    "        images = images.to(device)\n",
    "        \n",
    "        # Create labels for training discriminator and generator\n",
    "        real_labels = torch.ones(batch_size, 1).to(device)\n",
    "        fake_labels = torch.zeros(?, ?).to(?)\n",
    "        \n",
    "        # ====================================================================\n",
    "        # TRAINING DISCRIMINATOR\n",
    "        # ====================================================================\n",
    "        # Step1: Infer the discriminator on real images\n",
    "        outputs = ?\n",
    "        \n",
    "        # Step2: Calculate the loss with real labels\n",
    "        d_real_data_loss = criterion(?, ?)\n",
    "        real_score = outputs\n",
    "        \n",
    "        # Step3: Generate random latent vectors from normal distribution\n",
    "        z = torch.randn(batch_size, ?).to(?) \n",
    "        \n",
    "        # Step4: Generate fake images by passing latent vector to generator\n",
    "        fake_images = ?\n",
    "        \n",
    "        # Step5: Infer the discriminator on fake images\n",
    "        outputs = ?\n",
    "        \n",
    "        # Step6: Calculate the loss with fake labels\n",
    "        d_fake_data_loss = criterion(?, ?)\n",
    "        fake_score = outputs\n",
    "        \n",
    "        # Step7: Calculate the total loss\n",
    "        d_loss = ?\n",
    "        d_epoch_loss += d_loss.item()\n",
    "        \n",
    "        # Step8: Update the weights\n",
    "        optimizer_D.zero_grad()\n",
    "        d_loss.backward()\n",
    "        optimizer_D.step()\n",
    "        \n",
    "        \n",
    "        # ====================================================================\n",
    "        # TRAINING GENERATOR\n",
    "        # ====================================================================\n",
    "        \n",
    "        # Step1: Generate random latent vectors from normal distribution\n",
    "        z = ?\n",
    "        \n",
    "        # Step2: Generate fake images by passing latent vector to generator\n",
    "        fake_images = ?\n",
    "        \n",
    "        # Step3: Infer the discriminator on fake images\n",
    "        outputs = ?\n",
    "        \n",
    "        # Step4: Calculate the loss with real labels\n",
    "        g_loss = criterion(?, ?)\n",
    "        g_epoch_loss += g_loss.item()\n",
    "        \n",
    "        # Step5: Update the weights\n",
    "        optimizer_G.zero_grad()\n",
    "        g_loss.backward()\n",
    "        optimizer_G.step()\n",
    "        \n",
    "        if (i+1) % 200 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(trainloader)}], d_loss: {d_loss.item():.4f}, g_loss: {g_loss.item():.4f}, D(x): {real_score.mean().item():.2f}, D(G(z)): {fake_score.mean().item():.2f}')\n",
    "\n",
    "    generator_loss.append(g_epoch_loss / i)\n",
    "    discriminator_loss.append(d_epoch_loss / i)\n",
    "    # Visualize the intermediate results\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        fake_images = fake_images.reshape(fake_images.size(0), 1, 28, 28)\n",
    "        fake_images = fake_images.detach().cpu().numpy()\n",
    "        \n",
    "        fig, ax = plt.subplots(1, 10, figsize=(10, 1))\n",
    "        for i in range(10):\n",
    "            ax[i].imshow(fake_images[i, 0], cmap='gray')\n",
    "            ax[i].axis('off')\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell-9\n",
    "torch.save(generator, 'generator.pt')\n",
    "torch.save(discriminator, 'discriminator.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Learning Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell-10\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell-11\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Subplot for discriminator and generator loss\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Loss Plot\")\n",
    "train_num_epoch = [i + 1 for i in range(len(discriminator_loss))]\n",
    "plt.plot(train_num_epoch, discriminator_loss)\n",
    "plt.xlabel(\"#Epochs\")\n",
    "plt.ylabel(\"Discriminator Training Loss\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"Loss Plot\")\n",
    "train_num_epoch = [i + 1 for i in range(len(discriminator_loss))]\n",
    "plt.plot(train_num_epoch, generator_loss)\n",
    "plt.xlabel(\"#Epochs\")\n",
    "plt.ylabel(\"Generator Training Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Images by Inferencing Trained GAN\n",
    "\n",
    "* For inferencing, we need only Generator Network.\n",
    "\n",
    "* First we need to create a latent vector of normal distribution to feed it to generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell-12\n",
    "# Your code here\n",
    "\n",
    "# Load the model\n",
    "generator = ?\n",
    "\n",
    "# Set the generator to evaluation mode\n",
    "?\n",
    "\n",
    "# Generate latent vectors \n",
    "num_samples = 10\n",
    "input_noise = ? \n",
    "\n",
    "# Generate image using latent vectors\n",
    "generated_samples = ?\n",
    "\n",
    "generated_samples_np = generated_samples.detach().cpu().numpy()\n",
    "\n",
    "# Visualize the generated samples\n",
    "fig, axes = plt.subplots(1, num_samples, figsize=(15, 3))\n",
    "for i in range(num_samples):\n",
    "    axes[i].imshow(generated_samples_np[i, 0], cmap='gray')\n",
    "    axes[i].axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore Different GANs \n",
    "1. DCGANs\n",
    "2. Condition GANs\n",
    "3. Wasserstein GANs\n",
    "4. Progressive GANs\n",
    "5. StyleGANs\n",
    "6. CycleGANs\n",
    "7. StarGANs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "soccer-analytics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
